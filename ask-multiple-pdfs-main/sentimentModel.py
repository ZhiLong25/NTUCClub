# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18_F8M1Fo9Ax0OFg6EiInGvEL00yC1GP_
"""

from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import tensorflow as tf
from sklearn import preprocessing

Input = tf.keras.Input
Model = tf.keras.Model
with open("train.ft.txt", "r", encoding="utf-8") as trainingData:
    df = pd.DataFrame({"Text": trainingData.readlines()})

with open("test.ft.txt", "r", encoding="utf-8") as testingData:
    df_Test = pd.DataFrame({"Text": testingData.readlines()})
#training data set [3600000 rows x 1 columns]
df[['Label', 'Text']] = df['Text'].str.split(n=1, expand=True)
df_Test[['Label', 'Text']] = df['Text'].str.split(n=1, expand=True)
X_train,y_train = df['Text'],df['Label']
X_test,y_test = df['Text'],df['Label']

y_train_array = np.array(y_train)
if isinstance(y_train_array, tf.Tensor):
    y_train_array = y_train_array.numpy()
mlb = preprocessing.MultiLabelBinarizer()

y_train= mlb.fit_transform(y_train_array)
y_train = y_train[:,0]

token = tf.keras.preprocessing.text.Tokenizer(num_words=20000)
token.fit_on_texts(X_train)

# print(X_train)
np.shape(X_train)
token.get_config()

X_test = token.texts_to_sequences(X_test)
X_train = token.texts_to_sequences(X_train)

# Convert text sequences to integer sequences (tokenization)
# X_train_seq = token.texts_to_sequences(X_train)

# Pad sequences to ensure uniform length
max_seq_length = 50  # or any other desired sequence length
X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_seq_length, padding="pre", truncating="post")

v = len(token.word_index)

#input for embedding must be the total vocab size + 1
inputs = Input(shape = X_train[0].shape)
x = tf.keras.layers.Embedding(v+1,60)(inputs)
x1 = tf.keras.layers.SimpleRNN(128)(x)
x2 = tf.keras.layers.Flatten()(x1)
x3 = tf.keras.layers.Dense(128,activation="relu")(x2)
x4=tf.keras.layers.Dense(units=1,activation="sigmoid")(x3)
model = Model(inputs=inputs, outputs=x4)

type(X_train)

model.compile(optimizer="adam",loss="binary_crossentropy",metrics=["accuracy"])
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath='model_checkpoint.h5',
    save_best_only=True,  # Save only the best model
    monitor='accuracy',   # Monitor validation loss
    mode='min'            # Save the model when validation loss is minimized
)

# Define a callback for early stopping
early_stopping_callback = tf.keras.callbacks.EarlyStopping(
    monitor='accuracy',   # Monitor validation loss
    patience=3,            # Number of epochs with no improvement after which training will be stopped
    mode='min'            # Stop training when validation loss is no longer decreasing
)

model.fit(X_train,y_train,epochs=20,callbacks=[checkpoint_callback, early_stopping_callback])

max_seq_length = 50  # or any other desired sequence length
y_test_array = np.array(y_test)
if isinstance(y_test_array, tf.Tensor):
    y_test_array = y_test_array.numpy()
mlb = preprocessing.MultiLabelBinarizer()

y_test= mlb.fit_transform(y_test_array)
y_test = y_test[:,0]
X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_seq_length, padding="pre", truncating="post")

model.evaluate(X_test,y_test)
model.save("SemanticModel")